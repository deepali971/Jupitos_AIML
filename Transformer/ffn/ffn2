# Step 2: Fine-Tuning the Feedforward Neural Network (FNN) using Pre-trained BERT Embeddings

# Disable gradient updates for BERT (We only fine-tune FNN)
for param in model.bert.parameters():
    param.requires_grad = False

# Fine-tune only the Feedforward Network (FNN)
model.train()
for epoch in range(5):
    optimizer.zero_grad()
    # Extract embeddings without gradients
    with torch.no_grad():
        outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)
    # Extract [CLS] embedding
    cls_embedding = outputs.last_hidden_state[:, 0, :]
    # Pass embeddings through FNN
    predictions = model.ffn(cls_embedding).squeeze()
    # Calculate loss
    loss = criterion(predictions, labels)
    # Backward pass
    loss.backward()
    # Update FNN weights only
    optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

# In this approach, only the FNN gets fine-tuned, BERT remains frozen.