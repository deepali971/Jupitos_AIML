#3. Positional Encoding
#Sinusoidal Positional Encoding
import math

def positional_encoding(seq_length, d_model):
    pos_enc = torch.zeros(seq_length, d_model)
    for pos in range(seq_length):
        for i in range(0, d_model, 2):
            pos_enc[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))
            pos_enc[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / d_model)))
    return pos_enc

pos_enc_output = positional_encoding(10, 16)  # 10 tokens, 16 dimensions
print("Positional Encoding Matrix:")
print(pos_enc_output)
